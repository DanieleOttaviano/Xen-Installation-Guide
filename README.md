# Xen-Installation-Guide

In this reposity i explain how to test and install Xen on a Virtual Machine for testing and teching purpose.  

## Introduction

Xen is born as **Type-1** or bare-metal hypervisor for server, and it uses the technique of **Para-virtualization** due to which guest operating systems must be aware of being virtualized and must be modified to perform hypercalls to communicate with the underlying hypervisor. One of the main advantages of Xen, especially for the server environment, is the ability to take advantage of both para-virtualization and hardware support of new architectures to allow virtual machines to achieve performance very close to non-virtualized execution. 
Therefore, Xen turns out to be interesting as open-source software easily configurable and usable and characterized by a very small footprint and interface (about 1 Mb occupancy) due to the microkernel design used, which also makes it very reliable and safe. 
A VM that runs on Xen is called **domain**, and the most important domain is the so-called domain 0 (or **Dom0**). The latter is the first guestOS started at boot and it has drivers for all devices in the system. Moreover, Dom0 contains a control stack and other system services to manage a system based on XEN; it has functions to create virtual machines, destroy them, monitor them, etc. All other virtual machines run by XEN are called **DomU** (Unprivileged Domains) and generally do not have direct access to the hardware. Hence, they must rely on the hypervisor mechanisms and Dom0 to access the resources. 
To cope with the demands of the IIoT (Industrial Internet of Things) world of hypervisors able to manage execution environments with strict real-time constraints and mixed criticality issues, the Xen community has developed alternative schedulers for the management of virtual CPUs on physical CPUs and has reduced the latency generated by the management of interruptions.
The first real-time scheduler proposed by XEN is an implementation of the **Real-Time Deferrable Scheduler** (RTDS) to ensure a certain amount of CPU defined a priori to guestOS running on the system.

-	The RTDS algorithm assigns to each vCPU of each domain a budget and a period, such that each VM is supposed to execute its budget, even in a fragmented manner, in each period. The deadline corresponds to the end of each period and the budget is restored at the beginning of each period. Each vCPU must necessarily finish its budget by the end of the period, but in case there is no demand in a period or not enough to consume the entire budget, the unused part of the budget will be discarded at the end of the period. RTDS follows the theory of the Preemptive global Earliest Deadline First (EDF) to schedule the vCPU: At each point in time, the vCPU with the closest deadline will be scheduled. To schedule a vCPU means to run it on a physical CPU, and this happens, according to what is said, when a physical CPU is idle or when there is a lower priority task running on it.

Besides RTDS, Xen offers the possibility to also use the null scheduler: 

-	This scheduler assigns each vCPU to physical ones, ensuring their execution and removing the overhead of scheduling decisions at the cost of the entire use of the physical CPU chosen.

Xen also allows splitting the available CPUs into sets called pools characterized by different schedulers. This allows to run VMs with high throughput required but without real-time constraints on a pool that runs non-real-time scheduling algorithms, while VMs with time constraints run on a CPU pool with real-time scheduling (RTDS or null scheduler).

## References and further information

- What is Xen and how it works? https://wiki.xenproject.org/wiki/Xen_Project_Software_Overview
- Xen virtualization mode PV, HVM, PVHVM, and PVH and differences between them. https://wiki.xenproject.org/wiki/Understanding_the_Virtualization_Spectrum
- Xen Schedulers (Real-Time and non Real-Time). https://wiki.xenproject.org/wiki/Xen_Project_Schedulers#Use_cases_and_Support_Status
- Installation guide.	https://wiki.xenproject.org/wiki/Xen_Project_Beginners_Guide
- List of Xen Commands.	https://xenbits.xen.org/docs/unstable/man/xl.1.html
- RTDS scheduler.	https://xenbits.xen.org/docs/4.10-testing/features/sched_rtds.html
- Xen Best Practices. https://wiki.xenproject.org/wiki/Xen_Project_Best_Practices
- How to manage CPU pools? https://wiki.xenproject.org/wiki/Cpupools_Howto

## Index

In this tutorial we see:

- Preparation of the environment (VMware/VirtualBox etc…)
- Installation of Debian-11
- Installation of Xen on Debian
- Network Setup
- GRUB Setup
- LVM Setup for guests
- Creation of a PV Guest from scratch
- Debian PV Guest with xen-tools
- Setup of RTDS on Xen
- CPU pools
- Setup of NULL-SCHEDULER on Xen
- Fixed amount of memory for Xen Project dom0

## Preparation of the environment (VMware/VirtualBox etc…)

If you want to install Xen on a physical machine you can skip this part. I decided to install Xen on a Virtual Machine for teaching and portability purposes. Of course, tests done in virtual environments will not be reliable in terms of execution times due to the unpredictability of the entire software stack on which the hypervisor is running. But this is the easiest way to learn how to configure a bare-metal hypervisor. If you need to test the real delay of Xen solution you must install Xen on a physical machine.
First, to execute a virtual machine inside another virtual machine you must enable the Virtualization extensions. If you don’t know how to do it, you can follow the following guides:

- For VMware Workstation, and VirtualBox: https://www.tactig.com/enable-intel-vt-x-amd-virtualization-pc-vmware-virtualbox/ 
- For VMware Fusion: https://techgenix.com/vmware-fusion-5-enable-vt-xept-inside-a-virtual-machine-288/

Then, you must download and install a Virtual Machines Manager like Virtual Box, VMware or any other. 

-	VirtualBox download: https://www.virtualbox.org/. 
-	VMware download: https://www.vmware.com/it/products/workstation-player/workstation-player-evaluation.html

Once the Virtual Machine Manager is installed you need the most recent Debian ISO image, that you can find here: 

-	http://cdimage.debian.org/debian-cd/current/amd64/iso-cd/

Debian is a simple, stable, and well supported Linux distribution. It has included Xen Project Hypervisor support since Debian 3.1 “Sarge” released in 2005. It uses a simple Apt package management system which is both powerful and simple to use. (e.g., apt-get install <<name_of_the_package>>).
We have to create a new Debian Virtual Machine using the downloaded ISO (The example is done on VirtualBox, but it is similar on other VM Managers). 
Open VirtualBox, click on **New** and compile the widget. You must choice **Linux** as type and **Debian (64bit)** as Version. Then you can assign the quantity of RAM to the virtual machine. I decided to assign 4GB but the more you have the better! Remember that we will deploy several OS on this VM.

<p align="center">
    <img src="/../main/Images/environment1.png" width=50% height=50%>
</p>



Click Create. 
Now you must choice the quantity of disk reserved for the VM. As before, the more the better. I suggest you to not assign less than 40gb.

<p align="center">
<img src="/../main/Images/environment2.png" width=50% height=50%>
</p>


Click Create again. Finally, you can **start** the VM.
Once started the VM, you must choice the start-up disk. You can insert the ISO image we downloaded before. 

<p align="center">
<img src="/../main/Images/environment3.png" width=50% height=50%>
</p>


Click **Choose** and then **Start**.

## Installation of Debian-11

Once the VM is started you should see a menu, choose the default “**Install**” option to begin the installation process. Install the system The Debian installer is very straight forward. Follow the prompts until you reach the **disk partitioning section**.
Choose **advanced/custom**, we are going to configure a few partitions here, one for **Boot** “/boot” another for **RootFS** “/”, one more for **swap** and a final partition to setup as an **LVM** (Logical Volume Manager) volume group for our guest machines.

- First create the “/boot” partition by choosing the disk and hitting enter, make the partition 300MB and format it as ext2, choose /boot as the mountpoint.
- Repeat the process for “/” but of course changing the mountpoint to “/” and making it 15GB or so large. Format it as ext3.
- Create another partition approximately 1.5x the amount of RAM you have in size and elect to have it used as a swap volume (6GB in my case).
- Finally create a partition that consumes the rest of the diskspace and reserve it for LVM

We should now have a layout that looks like this assuming your disk device is /dev/sda :

    sda1 - /boot 200MB
    sda2 - / 15GB
    sda3 – swap 	6GB
    sda4 - reserved for LVM

When you reach the package selection stage only install the base system. We won’t require any GUI or other packages.
If you need, you can find a more detailed guide on Debian installation here: https://www.debian.org/releases/stable/amd64/ 

Finally, you should have a virtual machine with Debian running. Enter “root” as username and then the password you chose during the installation:

<p align="center">
<img src="/../main/Images/installation1.png" width=50% height=50%>
</p>

## Installation of Xen on Debian

We need to install the **Debian Xen Project** via an apt meta-package called xen-linux-system (A meta-package is basically a way of installing a group of packages automatically and Apt will of course resolve all dependencies and bring in all the extra libraries we need).

The Debian Xen Project packages consist primarily of a Xen Project-enabled Linux kernel, the hypervisor itself, a modified version of QEMU that support the hypervisor’s HVM mode and a set of userland tools. 

Install the xen-linux-system meta-package:

	apt-get install xen-system-amd64

Now we have a Xen Project hypervisor, a Xen Project kernel and the userland tools installed. When you next boot the system, the boot menu should include entries for starting Debian with the Xen hypervisor.

<p align="center">
<img src="/../main/Images/installation2.png" width=50% height=50%>
</p>

Logging in as root and launch the following command to see the Xen section of **dmesg** created during the boot process. (dmesg prints on the standard output the messages stored inside the buffer of the OS kernel)

```
 xl dmesg
```

**xl** is the Xen Project management tool, based on LibXenlight. You can launch xl to see the existing commands:

```
 xl
```

(For further information about xl read: http://xenbits.xen.org/docs/4.11-testing/man/xl.1.html.)

<p align="center">
<img src="/../main/Images/installation3.png" width=50% height=50%>
</p>

Now you can check If the virtualization is enabled in the bios searching for the strings vmx or svm or hypervisor in /proc/cpuinfo:

```
 egrep '(vmx|svm|hypervisor)' /proc/cpuinfo 
```

<p align="center">
<img src="/../main/Images/installation4.png" width=50% height=50%>
</p>

## Network Setup

We need to set up our system so that we can attach virtual machines to the external network. This is done by creating a virtual switch within dom0. The switch will take packets from the virtual machines and forward them on to the physical network so they can see the internet and other machines on your network.

The piece of software we use to do this is called the **Linux bridge** and its core components already reside inside the Linux kernel. In this case, the bridge acts as our virtual switch. The Debian kernel is compiled with the Linux bridging module so all we need to do is install the control utilities:

```
 apt-get install bridge-utils
```

To configure the network, we need to modify the interface file (I’m going to use nano as text editor, but you can use anything you want): 

```
 nano /etc/network/interfaces
```

Depending on your hardware you see a file like this:

<p align="center">
<img src="/../main/Images/network1.png" width=30% height=30%>
</p>

Each stanza represents a single interface, let’s analyze the second:

- “allow-hotplug ens33” means that *ens33* will be configured when *ifup -a* is run, which happens at boot time (*ifup* and *ifdown* are scripts used to activate and deactivate network interfaces). This means that the interface will automatically be started/stopped for you.

-  “iface ens33” describes the interface itself. In this case, it specifies that it should be configured by DHCP 

for further information on the Linux network interface file you can read: 

- https://qastack.it/unix/128439/good-detailed-explanation-of-etc-network-interfaces-syntax (ITA)
- https://unix.stackexchange.com/questions/128439/good-detailed-explanation-of-etc-network-interfaces-syntax (ENG)

You must edit the file, so it resembles such:

<p align="center">
<img src="/../main/Images/network2.png" width=30% height=30%>
</p>

In this way, we assign the IP address to the bridged interface. Now restart networking:

```
service networking restart
```

To check if it worked launch this command:

```
brctl show
```

If all is well, the bridge will be listed, and your interface will appear in the interfaces column: 

<p align="center">
<img src="/../main/Images/network3.png" width=60% height=60%>
</p>

Bridged networking will now automatically start every boot.



## Grub Setup

GRUB (GRand Unified Bootloader) is the bootloader installed during the installation of Debian. It tells the computer which OS to start and how. To use the hypervisor, Xen must be started before the operating system. The information about which OS to start first can be found in /boot/grub/grub.cfg but we don’t want to change it manually. This is because it changes every time the kernel is updated. 

Thankfully, Debian configures GRUB for us using a number of automated scripts that handle upgrades. These scripts are stored in /etc/grub.d/* .To configure these script we can modify the file grub: 

```
/etc/default/grub
```

If XEN is the third choice in the GRUB menu, you will change the string GRUB_DEFAULT=0 into GRUB_DEFAULT=2 to have Xen load by default.

Then regenerate the /boot/grub/grub.cfg file by running: 

```
/usr/sbin/grub-mkconfig -o /boot/grub/grub.cfg
```

or

```
update-grub
```

And reboot the system to verify that the change has been made. 

For further information about GRUB read: https://opensource.com/article/17/3/introduction-grub2-configuration-linux



## LVM Setup for guests

LVM is the Linux *Logical Volume Manager*. It is a technology that allows Linux to manage *block devices* in a more abstract manner. Each “**logical volume**” (lv) is a virtualized block composed of blocks written to one or more physical devices. Unlike the classical disk partition, these blocks don’t need to be continuous. (Because of this abstraction, logical volumes can be created, deleted, resized, and even snapshotted without affecting other logical volumes.)

Logical volumes are created inside the “**volume group**”, which is a set of logical volumes associated to the same physical storage, known as **physical volumes**.

The idea is to create a volume group on top of a physical volume, and then create a series of logical volumes on top of the volume group previously created.

First, we need to install LVM:

```
apt-get install lvm2
```

After the installation is completed, we can configure a physical device to maintain a volume group (in this example we use /dev/sda4):

```
pvcreate /dev/sda4
```

Now LVM has somewhere to store its blocks, so we can create a volume group using this physical volume (In this example is called “vg0”)

```
vgcreate vg0 /dev/sda4
```

Now that the volume group is created, is ready to maintain several *logical volumes*. 

To create a *logical volume* for a VM we will use the command lvcreate: 

```
lvcreate -n <name of the volume> -L <size, you can use G/M here> <volume group>
```

It is possible to remove the created volume with the following command:

```
lvremove /dev/vg0/database-data
```

You can list the volume groups with the following command :

```
vgs
```

You can list the local volumes with the following command:

```
lvs
```



## Creation of a PV Guest from scratch

There are several ways to launch a new Para-virtualized Virtual Machine on Xen. In this guide we see how to do it from scratch to give a better understanding of all the configurations.

First, we need a local volume partition for our VM (we assume that the volume group "vg0” has already been created as descried in the previous section). Let’s suppose that we want to run an ubuntu VM:

```
lvcreate -L 5G -n lv_vm_ubuntu /dev/vg0
```

Then, download the netboot image for Ubuntu 18.04 (In the example they are stored in /root/): 

```
wget http://archive.ubuntu.com/ubuntu/dists/bionic-updates/main/installer-amd64/current/images/netboot/xen/vmlinuz
wget http://archive.ubuntu.com/ubuntu/dists/bionic-updates/main/installer-amd64/current/images/netboot/xen/initrd.gz
```

To create the VM, we will use the command “xl create”. This command needs to parse a configuration file that we can write manually. Go into the path /etc/xen/ and create a file .cfg (in my example is called “ubuntu_vm_example.cfg”). Open the file with a text editor and paste this configuration (pay attention to the current Xen version in the last line).

<p align="center">
<img src="/../main/Images/PV1.png" width=60% height=60%>
</p>

Once the configuration is created and the netboot image is downloaded, we can finally create the VM:

```
xl create -c /etc/xen/ubuntu_vm_example.cfg
```

The -c in this command tells xl that we wish to connect to the guest virtual console, a paravirtualized serial port within the domain that xen-create-image configured to listen with a getty. This command also starts the VM.

You can leave the guest virtual console by pressing “**ctrl+]”** and re-enter it by running the “**xl console ubuntu_vm**” command.

You can later shutdown this guest either from within the domain or from dom0 with the following:

```
xl shutdown ubuntu_vm
```

After the first installation of ubuntu is completed, you must modify the configuration file “/etc/xen/ubuntu_vm_example.cfg”, otherwise every time you launch the “*xl create*” command the installation will run again. Instead, we want to boot the VM from the virtual disk and to do it you must comment the *kernel* and *ramdisk* option in the configuration file, and you must remove the comment on bootloader option. (OSS: Check the installed Xen version)

<p align="center">
<img src="/../main/Images/PV2.png" width=40% height=40%>
</p>

### Getting the VM IP

After creating VM and installing OS, you can recover the mac address of your VM with: 

```
xl network-list ubuntu_vm
```

<p align="center">
<img src="/../main/Images/PV3.png" width=60% height=60%>
</p>

Then you can use the **tcdumb** program to listen on bridge interface: 

```
tcpdump -n -i ens33 ether src 00:16:3e:57:db:c5
```

Obviously to have something in output the VM should use the network interface. So, enter in the VM with the command “xl console ubuntu_vm” and ping something. For example, launch “ping google.com”. You should see in the Dom0 something like this. 

<p align="center">
<img src="/../main/Images/PV4.png" width=60% height=60%>
</p>

You can notice that 192.168.128.136 is the IP of PV Guest.



## Debian PV Guest with xen-tools

Debian contains several tools for creating Xen Project guests, the easiest of which is known as **xen-tools** (https://wiki.xenproject.org/wiki/Xen-tools). This software suite manages the downloading and installing of guest operating systems including both Debian and RHEL based DomUs. In this guide we are going to use xen-tools to prepare a Debian paravirtualized domU.

We are going to use “vg0” as volume to create this VM as we did before. When guests are paravirtualized there is no “BIOS” or bootloader resident within the guest filesystem and for a long time guests were provided with kernels external to the guest image. This however is bad for maintainability (guests cannot upgrade their kernels without access to the dom0) and is not as flexible in terms of boot options as they must be passed via the config file.

The Xen Project community wrote a utility known as pygrub which is a python application for PV guests that enables the dom0 to parse the GRUB configuration of the domU and extract its kernel, initrd and boot parameters. This allows for kernel upgrades etc inside of our guest machines along with a GRUB menu. Using pygrub or the stub-dom implementation known as pv-grub is best practice for starting PV guests. In some cases pv-grub is arguably more secure but as it is not included with Debian we won’t use it here though it is recommended in production environments where guests cannot be trusted.

First, lets install the xen-tools

```
apt-get install xen-tools
```

Then it is possible to see all the available distro in the following path: 

```
ls /usr/share/xen-tools   
```

We can now create a guest operating system with this tool. It automates the process of setting up a PC guest from scratch right to the point of creating files and starting the guest. The process can be summarized as follows:

- Create logical volume for rootfs

- Create logical volume for swap

- Create filesystem for rootfs

- Mount rootfs

- Install operating system using debootstrap (or rinse etc, only debootstrap covered here)

- Run a series of scripts to generate guest config files like fstab/inittab/menu.lst

- Create a VM config file for the guest

- Generate a root password for the guest system

-  Unmount the guest filesystem

These 9 steps can be carried out manually but they can be executed automatically with the following:

```
xen-create-image --hostname=debian-pv-guest \
--memory=512mb \
--vcpus=2 \
--lvm=vg0 \
--dhcp \
--pygrub \
--dist=squeeze
```

This command instructs *xen-create-image* (the primary binary of the xen-tools toolkit) to create a guest domain with 512MB of memory, 2 vcpus, using storage from the vg0 volume group we created, use DHCP for networking, pygrub to extract the kernel from the image when booted and lastly, we specify that we want to deploy a Debian Squeeze operating system.

This process will take a few minutes. Once it is complete, it will provide a summary of the installation. 

**OSS: Take note of the root password for the guest.**



## Setup of RTDS on Xen

Once Xen is installed, the default scheduler is credit2 (https://wiki.xenproject.org/wiki/Credit2_Scheduler), that is a general purpose, weighted fair share scheduler. It is used to scheduler the vCPU (virtual CPU) on the pCPU (physical CPU) with the aim of reaching high throughput. 

It is possible to see the currently used scheduler in Xen with the command: 

```
xl info   
```

<p align="center">
<img src="/../main/Images/RTDS1.png" width=50% height=50%>
</p>

This kind of scheduler is not suitable for Real-Time. Hence, we can change the scheduler in favour of RTDS from the configuration file /etc/default/grub.d/xen.cfg.

```
 nano /etc/default/grub.d/xen.cfg  
```

Just enter the following strings:

<p align="center">
<img src="/../main/Images/RTDS2.png" width=50% height=50%>
</p>

Update Grub to make the changes effective:

```
update-grub  
```

if you reboot the system and run the command xl info, you should see the rtds scheduler:

<p align="center">
<img src="/../main/Images/RTDS3.png" width=50% height=50%>
</p>

Once RTDS is configured you can modify the parameters assigned to a VM with the following command:

```
xl sched-rtds
```

- -d DOMAIN, --domain=DOMAIN
  - Specify domain for which scheduler parameters are to be modified or retrieved. Mandatory for modifying scheduler parameters.
- -v VCPUID/all, --vcpuid=VCPUID/all
  - Specify vcpu for which scheduler parameters are to be modified or retrieved.
- -p PERIOD, --period=PERIOD
  - Period of time, in microseconds, over which to replenish the budget.
- -b BUDGET, --budget=BUDGET
  - Amount of time, in microseconds, that the VCPU will be allowed to run every period. 
- -e Extratime, --extratime=Extratime
  - Binary flag to decide if the VCPU will be allowed to get extra time from the unreserved system resource.
- -c CPUPOOL, --cpupool=CPUPOOL
  - Restrict output to domains in the specified cpupool.

As an example, we can assign to our ubuntu VM a period of 500us and budget 250us:

```
xl sched-rtds -d ubuntu_vm -v all -p 100000 -b 50000 -e 1
```

It exists an alternative way to use RTDS: Once the system is live, we can create a ***cpupool\*** with RTDS as its scheduler with the following command:

```
xl cpupool-create name=\"pool-rt\" sched=\"rtds\" cpus=[3]
```

In the next section concept of CPU pools is deepened (Probably if you try to execute the below command you will see the error: “cpu X illegal or not free”).  

To check if the scheduler is working fine you can:

- Configure RTDS as scheduler. 

- Create two VMs with 1 vCPU and associated to the same pCPU.

- set the scheduling parameters such as it has a 50% reservation, with xl sched-rtds 

-  run a CPU-burning process inside the VMs (e.g., yes).

- check with xl top (in Domain0) that the VM is getting no more than 50% pCPU time.

Let’s do it:

For testing purpose, I have created two ubuntu VM (see section “Creation of a PV Guest from scratch”) and I have assigned the same period and budget to both (xl sched-rtds -d ubuntu_vm -v all -p 100000 -b 50000 -e 1). Then I have modified the configuration files of both the VMs giving to each VM just 1 vCPU and assigning both the vCPUs to the same pCPU. (In my case the files are /etc/xen/ubuntu_vm_example.cfg and /etc/xen/ubuntu_vm2_xample.cfg) 

<p align="center">
<img src="/../main/Images/RTDS4.png" width=50% height=50%>
</p>

where <<cpus=”3”>> means that the vCPU is assigned to the fourth pCPU (the value “cpus” starts from 0).

If we put both the VMs under stress and monitor the cpu utilization with the command “*xl top”,* it is possible to see that the utilization is almost 50% for both the VMs as expected

<p align="center">
<img src="/../main/Images/RTDS5.png" width=50% height=50%>
</p>

OSS: it is not exactly 50% because of the unpredictability of the software stack on which Xen is running. The test should be repeated on a bare metal environment to provide reliable results.  

